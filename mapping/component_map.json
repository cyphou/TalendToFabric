{
  "description": "Maps Talend components to their Fabric Data Factory / Spark equivalents",
  "version": "2.0",
  "mappings": {
    "_comment_db_inputs": "=== DATABASE INPUT COMPONENTS ===",
    "tOracleInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "OracleSource",
      "spark": "spark.read.jdbc()",
      "notes": "Oracle source — in Oracle→PostgreSQL migration, replace with PostgreSQL source"
    },
    "tPostgresqlInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "AzurePostgreSqlSource",
      "spark": "spark.read.jdbc()",
      "notes": "Direct mapping to Azure PostgreSQL source"
    },
    "tMSSqlInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "SqlServerSource",
      "spark": "spark.read.jdbc()",
      "notes": "SQL Server source"
    },
    "tMysqlInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "MySqlSource",
      "spark": "spark.read.jdbc()",
      "notes": "MySQL source"
    },
    "tDB2Input": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "Db2Source",
      "spark": "spark.read.jdbc()",
      "notes": "DB2 source"
    },
    "tTeradataInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "TeradataSource",
      "spark": "spark.read.jdbc()",
      "notes": "Teradata source"
    },
    "tSybaseInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "SapBwSource",
      "spark": "spark.read.jdbc()",
      "notes": "Sybase / SAP ASE source"
    },
    "tRedshiftInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "AmazonRedshiftSource",
      "spark": "spark.read.jdbc()",
      "notes": "Redshift source"
    },
    "tSnowflakeInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "SnowflakeSource",
      "spark": "spark.read.format('snowflake')",
      "notes": "Snowflake source"
    },
    "tHiveInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "HiveSource",
      "spark": "spark.sql('SELECT ...')",
      "notes": "Hive source — native to Spark"
    },
    "tDBInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "OdbcSource",
      "spark": "spark.read.jdbc()",
      "notes": "Generic DB input via JDBC/ODBC"
    },
    "tSQLiteInput": {
      "category": "db_input",
      "data_factory": "NotSupported",
      "spark": "spark.read.jdbc()",
      "notes": "SQLite — use JDBC driver"
    },
    "tInformixInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "InformixSource",
      "spark": "spark.read.jdbc()",
      "notes": "Informix source"
    },

    "_comment_db_outputs": "=== DATABASE OUTPUT COMPONENTS ===",
    "tOracleOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "OracleSink",
      "spark": "df.write.jdbc()",
      "notes": "Oracle target — replace with PostgreSQL or Lakehouse target"
    },
    "tPostgresqlOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "AzurePostgreSqlSink",
      "spark": "df.write.jdbc()",
      "notes": "Direct mapping to Azure PostgreSQL sink"
    },
    "tMSSqlOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "SqlServerSink",
      "spark": "df.write.jdbc()",
      "notes": "SQL Server sink"
    },
    "tMysqlOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "MySqlSink",
      "spark": "df.write.jdbc()",
      "notes": "MySQL sink"
    },
    "tDB2Output": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "Db2Sink",
      "spark": "df.write.jdbc()",
      "notes": "DB2 sink"
    },
    "tTeradataOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "TeradataSink",
      "spark": "df.write.jdbc()",
      "notes": "Teradata sink"
    },
    "tRedshiftOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "AmazonRedshiftSink",
      "spark": "df.write.jdbc()",
      "notes": "Redshift sink"
    },
    "tSnowflakeOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "SnowflakeSink",
      "spark": "df.write.format('snowflake')",
      "notes": "Snowflake sink"
    },
    "tHiveOutput": {
      "category": "db_output",
      "data_factory": "NotApplicable",
      "spark": "df.write.saveAsTable()",
      "notes": "Hive output — native to Spark Lakehouse"
    },
    "tDBOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "OdbcSink",
      "spark": "df.write.jdbc()",
      "notes": "Generic DB output via JDBC/ODBC"
    },

    "_comment_db_ops": "=== DATABASE OPERATION COMPONENTS ===",
    "tOracleConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Oracle",
      "spark": "JDBC connection properties",
      "notes": "Oracle connection definition"
    },
    "tPostgresqlConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_AzurePostgreSQL",
      "spark": "JDBC connection properties",
      "notes": "PostgreSQL connection definition"
    },
    "tMSSqlConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_SqlServer",
      "spark": "JDBC connection properties",
      "notes": "SQL Server connection definition"
    },
    "tMysqlConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_MySql",
      "spark": "JDBC connection properties",
      "notes": "MySQL connection definition"
    },
    "tDBConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Odbc",
      "spark": "JDBC connection properties",
      "notes": "Generic DB connection"
    },
    "tOracleCommit": {
      "category": "db_operation",
      "data_factory": "AutoCommit",
      "spark": "Explicit commit / auto in Spark",
      "notes": "Transaction commit"
    },
    "tDBCommit": {
      "category": "db_operation",
      "data_factory": "AutoCommit",
      "spark": "Explicit commit / auto in Spark",
      "notes": "Transaction commit"
    },
    "tOracleRollback": {
      "category": "db_operation",
      "data_factory": "ErrorHandler",
      "spark": "try-except with rollback",
      "notes": "Transaction rollback"
    },
    "tDBRollback": {
      "category": "db_operation",
      "data_factory": "ErrorHandler",
      "spark": "try-except with rollback",
      "notes": "Transaction rollback"
    },
    "tOracleSP": {
      "category": "db_operation",
      "data_factory": "StoredProcedureActivity",
      "spark": "spark.sql('CALL ...')",
      "notes": "Stored procedure execution"
    },
    "tDBSP": {
      "category": "db_operation",
      "data_factory": "StoredProcedureActivity",
      "spark": "spark.sql('CALL ...')",
      "notes": "Stored procedure execution"
    },
    "tDBRow": {
      "category": "db_operation",
      "data_factory": "ScriptActivity",
      "spark": "spark.sql('INSERT/UPDATE/DELETE ...')",
      "notes": "Execute arbitrary SQL statement"
    },
    "tDBBulkExec": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "df.write.jdbc(batchsize=N)",
      "notes": "Bulk data loading"
    },
    "tOracleBulkExec": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "df.write.jdbc(batchsize=N)",
      "notes": "Oracle bulk data loading"
    },
    "tCreateTable": {
      "category": "db_operation",
      "data_factory": "ScriptActivity_DDL",
      "spark": "spark.sql('CREATE TABLE ...')",
      "notes": "DDL create table"
    },
    "tDropTable": {
      "category": "db_operation",
      "data_factory": "ScriptActivity_DDL",
      "spark": "spark.sql('DROP TABLE ...')",
      "notes": "DDL drop table"
    },
    "tAlterTable": {
      "category": "db_operation",
      "data_factory": "ScriptActivity_DDL",
      "spark": "spark.sql('ALTER TABLE ...')",
      "notes": "DDL alter table"
    },
    "tDBTableList": {
      "category": "db_operation",
      "data_factory": "GetMetadataActivity",
      "spark": "spark.catalog.listTables()",
      "notes": "List database tables"
    },
    "tDBColumnList": {
      "category": "db_operation",
      "data_factory": "GetMetadataActivity",
      "spark": "spark.catalog.listColumns()",
      "notes": "List table columns"
    },
    "tOracleClose": {
      "category": "db_operation",
      "data_factory": "Automatic",
      "spark": "Connection auto-close",
      "notes": "Close connection"
    },
    "tDBClose": {
      "category": "db_operation",
      "data_factory": "Automatic",
      "spark": "Connection auto-close",
      "notes": "Close connection"
    },
    "tDBValidation": {
      "category": "db_operation",
      "data_factory": "GetMetadataActivity",
      "spark": "spark.catalog.tableExists()",
      "notes": "Validate table existence"
    },

    "_comment_file_inputs": "=== FILE INPUT COMPONENTS ===",
    "tFileInputDelimited": {
      "category": "file_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "DelimitedTextSource",
      "spark": "spark.read.csv()",
      "notes": "CSV file input"
    },
    "tFileInputExcel": {
      "category": "file_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "ExcelSource",
      "spark": "spark.read.format('com.crealytics.spark.excel')",
      "notes": "Excel file input"
    },
    "tFileInputJSON": {
      "category": "file_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "JsonSource",
      "spark": "spark.read.json()",
      "notes": "JSON file input"
    },
    "tFileInputParquet": {
      "category": "file_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "ParquetSource",
      "spark": "spark.read.parquet()",
      "notes": "Parquet file input"
    },
    "tFileInputXML": {
      "category": "file_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "XmlSource",
      "spark": "spark.read.format('xml')",
      "notes": "XML file input"
    },
    "tFileInputAvro": {
      "category": "file_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "AvroSource",
      "spark": "spark.read.format('avro')",
      "notes": "Avro file input"
    },
    "tFileInputORC": {
      "category": "file_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "OrcSource",
      "spark": "spark.read.orc()",
      "notes": "ORC file input"
    },
    "tFileInputPositional": {
      "category": "file_input",
      "data_factory": "CopyActivity_Custom",
      "spark": "spark.read.text() + substring parsing",
      "notes": "Fixed-width file — requires custom column extraction"
    },
    "tFileInputRegex": {
      "category": "file_input",
      "data_factory": "NotSupported",
      "spark": "spark.read.text() + regexp_extract()",
      "notes": "Regex-based parsing — Spark only"
    },
    "tFileInputFullRow": {
      "category": "file_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "BinarySource",
      "spark": "spark.read.text()",
      "notes": "Read entire row as single field"
    },
    "tFileInputRaw": {
      "category": "file_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "BinarySource",
      "spark": "spark.sparkContext.binaryFiles()",
      "notes": "Raw binary file input"
    },
    "tFileInputLDIF": {
      "category": "file_input",
      "data_factory": "NotSupported",
      "spark": "Custom Python parser",
      "notes": "LDIF file — requires custom parsing"
    },

    "_comment_file_outputs": "=== FILE OUTPUT COMPONENTS ===",
    "tFileOutputDelimited": {
      "category": "file_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "DelimitedTextSink",
      "spark": "df.write.csv()",
      "notes": "CSV file output"
    },
    "tFileOutputExcel": {
      "category": "file_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "ExcelSink",
      "spark": "df.write.format('com.crealytics.spark.excel')",
      "notes": "Excel file output"
    },
    "tFileOutputJSON": {
      "category": "file_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "JsonSink",
      "spark": "df.write.json()",
      "notes": "JSON file output"
    },
    "tFileOutputParquet": {
      "category": "file_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "ParquetSink",
      "spark": "df.write.parquet()",
      "notes": "Parquet file output"
    },
    "tFileOutputXML": {
      "category": "file_output",
      "data_factory": "DataFlow_XmlSink",
      "spark": "df.write.format('xml')",
      "notes": "XML file output"
    },
    "tFileOutputAvro": {
      "category": "file_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "AvroSink",
      "spark": "df.write.format('avro')",
      "notes": "Avro file output"
    },
    "tFileOutputORC": {
      "category": "file_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "OrcSink",
      "spark": "df.write.orc()",
      "notes": "ORC file output"
    },
    "tFileOutputPositional": {
      "category": "file_output",
      "data_factory": "NotSupported",
      "spark": "format_string() + df.write.text()",
      "notes": "Fixed-width output — custom formatting"
    },

    "_comment_cloud_storage": "=== CLOUD STORAGE COMPONENTS ===",
    "tS3Connection": {
      "category": "cloud_storage",
      "data_factory": "LinkedService_AmazonS3",
      "spark": "S3A configuration",
      "notes": "AWS S3 connection"
    },
    "tS3Input": {
      "category": "cloud_storage",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "AmazonS3Source",
      "spark": "spark.read.csv('s3a://...')",
      "notes": "Read from S3"
    },
    "tS3Output": {
      "category": "cloud_storage",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "AmazonS3Sink",
      "spark": "df.write.csv('s3a://...')",
      "notes": "Write to S3"
    },
    "tS3Get": {
      "category": "cloud_storage",
      "data_factory": "CopyActivity",
      "spark": "boto3.download_file()",
      "notes": "Download from S3"
    },
    "tS3Put": {
      "category": "cloud_storage",
      "data_factory": "CopyActivity",
      "spark": "boto3.upload_file()",
      "notes": "Upload to S3"
    },
    "tAzureBlobInput": {
      "category": "cloud_storage",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "AzureBlobStorageSource",
      "spark": "spark.read.csv('wasbs://...')",
      "notes": "Read from Azure Blob"
    },
    "tAzureBlobOutput": {
      "category": "cloud_storage",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "AzureBlobStorageSink",
      "spark": "df.write.csv('wasbs://...')",
      "notes": "Write to Azure Blob"
    },
    "tAzureBlobConnection": {
      "category": "cloud_storage",
      "data_factory": "LinkedService_AzureBlobStorage",
      "spark": "wasbs:// or abfss:// configuration",
      "notes": "Azure Blob connection"
    },
    "tAzureDataLakeInput": {
      "category": "cloud_storage",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "AzureDataLakeStoreSource",
      "spark": "spark.read.csv('abfss://...')",
      "notes": "Read from ADLS"
    },
    "tAzureDataLakeOutput": {
      "category": "cloud_storage",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "AzureDataLakeStoreSink",
      "spark": "df.write.csv('abfss://...')",
      "notes": "Write to ADLS"
    },
    "tGCSInput": {
      "category": "cloud_storage",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "GoogleCloudStorageSource",
      "spark": "spark.read.csv('gs://...')",
      "notes": "Read from Google Cloud Storage"
    },
    "tGCSOutput": {
      "category": "cloud_storage",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "GoogleCloudStorageSink",
      "spark": "df.write.csv('gs://...')",
      "notes": "Write to Google Cloud Storage"
    },
    "tHDFSInput": {
      "category": "cloud_storage",
      "data_factory": "NotApplicable",
      "spark": "spark.read.csv('hdfs://...')",
      "notes": "Read from HDFS"
    },
    "tHDFSOutput": {
      "category": "cloud_storage",
      "data_factory": "NotApplicable",
      "spark": "df.write.csv('hdfs://...')",
      "notes": "Write to HDFS"
    },

    "_comment_nosql": "=== NOSQL / DOCUMENT DB COMPONENTS ===",
    "tMongoDBInput": {
      "category": "nosql",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "MongoDbSource",
      "spark": "spark.read.format('mongo')",
      "notes": "MongoDB source"
    },
    "tMongoDBOutput": {
      "category": "nosql",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "MongoDbSink",
      "spark": "df.write.format('mongo')",
      "notes": "MongoDB sink"
    },
    "tMongoDBConnection": {
      "category": "nosql",
      "data_factory": "LinkedService_MongoDB",
      "spark": "MongoDB connection string",
      "notes": "MongoDB connection"
    },
    "tCassandraInput": {
      "category": "nosql",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "CassandraSource",
      "spark": "spark.read.format('org.apache.spark.sql.cassandra')",
      "notes": "Cassandra source"
    },
    "tCassandraOutput": {
      "category": "nosql",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "CassandraSink",
      "spark": "df.write.format('org.apache.spark.sql.cassandra')",
      "notes": "Cassandra sink"
    },
    "tCosmosDBInput": {
      "category": "nosql",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "CosmosDbSqlApiSource",
      "spark": "spark.read.format('cosmos.oltp')",
      "notes": "Azure Cosmos DB source — recommended for AI/chat/RAG workloads"
    },
    "tCosmosDBOutput": {
      "category": "nosql",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "CosmosDbSqlApiSink",
      "spark": "df.write.format('cosmos.oltp')",
      "notes": "Azure Cosmos DB sink — use singleton CosmosClient, handle 429 with retry-after"
    },
    "tCouchDBInput": {
      "category": "nosql",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "CouchbaseSource",
      "spark": "requests + spark.createDataFrame()",
      "notes": "CouchDB source"
    },
    "tNeo4jInput": {
      "category": "nosql",
      "data_factory": "NotSupported",
      "spark": "neo4j Python driver",
      "notes": "Neo4j graph DB — requires custom connector"
    },
    "tNeo4jOutput": {
      "category": "nosql",
      "data_factory": "NotSupported",
      "spark": "neo4j Python driver",
      "notes": "Neo4j graph DB — requires custom connector"
    },
    "tDynamoDBInput": {
      "category": "nosql",
      "data_factory": "NotSupported",
      "spark": "boto3 + spark.createDataFrame()",
      "notes": "DynamoDB — requires custom connector"
    },
    "tDynamoDBOutput": {
      "category": "nosql",
      "data_factory": "NotSupported",
      "spark": "boto3 DynamoDB client",
      "notes": "DynamoDB — requires custom connector"
    },
    "tElasticsearchInput": {
      "category": "nosql",
      "data_factory": "NotSupported",
      "spark": "elasticsearch-spark connector",
      "notes": "Elasticsearch source"
    },
    "tElasticsearchOutput": {
      "category": "nosql",
      "data_factory": "NotSupported",
      "spark": "elasticsearch-spark connector",
      "notes": "Elasticsearch sink"
    },
    "tHBaseInput": {
      "category": "nosql",
      "data_factory": "NotApplicable",
      "spark": "spark.read.format('org.apache.hadoop.hbase.spark')",
      "notes": "HBase source"
    },
    "tRedisInput": {
      "category": "nosql",
      "data_factory": "NotSupported",
      "spark": "redis Python library",
      "notes": "Redis source"
    },
    "tRedisOutput": {
      "category": "nosql",
      "data_factory": "NotSupported",
      "spark": "redis Python library",
      "notes": "Redis sink"
    },

    "_comment_messaging": "=== MESSAGING & STREAMING COMPONENTS ===",
    "tKafkaInput": {
      "category": "messaging",
      "data_factory": "NotSupported_UseEventHub",
      "spark": "spark.readStream.format('kafka')",
      "notes": "Kafka consumer — use Event Hub in Azure or Spark Structured Streaming"
    },
    "tKafkaOutput": {
      "category": "messaging",
      "data_factory": "NotSupported_UseEventHub",
      "spark": "df.writeStream.format('kafka')",
      "notes": "Kafka producer"
    },
    "tKafkaConnection": {
      "category": "messaging",
      "data_factory": "LinkedService_EventHub",
      "spark": "Kafka bootstrap.servers configuration",
      "notes": "Kafka connection"
    },
    "tJMSInput": {
      "category": "messaging",
      "data_factory": "NotSupported",
      "spark": "stomp / jms Python library",
      "notes": "JMS consumer"
    },
    "tJMSOutput": {
      "category": "messaging",
      "data_factory": "NotSupported",
      "spark": "stomp / jms Python library",
      "notes": "JMS producer"
    },
    "tActiveMQInput": {
      "category": "messaging",
      "data_factory": "NotSupported",
      "spark": "stomp Python library",
      "notes": "ActiveMQ consumer"
    },
    "tActiveMQOutput": {
      "category": "messaging",
      "data_factory": "NotSupported",
      "spark": "stomp Python library",
      "notes": "ActiveMQ producer"
    },
    "tRabbitMQInput": {
      "category": "messaging",
      "data_factory": "NotSupported",
      "spark": "pika Python library",
      "notes": "RabbitMQ consumer"
    },
    "tRabbitMQOutput": {
      "category": "messaging",
      "data_factory": "NotSupported",
      "spark": "pika Python library",
      "notes": "RabbitMQ producer"
    },
    "tAzureEventHubInput": {
      "category": "messaging",
      "data_factory": "EventHubConnector",
      "spark": "spark.readStream.format('eventhubs')",
      "notes": "Azure Event Hub consumer"
    },
    "tAzureEventHubOutput": {
      "category": "messaging",
      "data_factory": "EventHubConnector",
      "spark": "df.writeStream.format('eventhubs')",
      "notes": "Azure Event Hub producer"
    },

    "_comment_api_protocol": "=== API & PROTOCOL COMPONENTS ===",
    "tRESTClient": {
      "category": "api",
      "data_factory": "WebActivity_CopyActivity_REST",
      "spark": "requests library + spark.createDataFrame()",
      "notes": "REST API call"
    },
    "tHTTPInput": {
      "category": "api",
      "data_factory": "WebActivity_CopyActivity_HTTP",
      "spark": "requests / urllib3",
      "notes": "HTTP GET/download"
    },
    "tHTTPClient": {
      "category": "api",
      "data_factory": "WebActivity",
      "spark": "requests library",
      "notes": "HTTP client"
    },
    "tSOAP": {
      "category": "api",
      "data_factory": "WebActivity_SOAP",
      "spark": "zeep / requests with XML payload",
      "notes": "SOAP web service call"
    },
    "tWebServiceInput": {
      "category": "api",
      "data_factory": "WebActivity",
      "spark": "zeep / requests",
      "notes": "Web service consumer"
    },
    "tSalesforceInput": {
      "category": "api",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "SalesforceSource",
      "spark": "simple-salesforce library",
      "notes": "Salesforce source"
    },
    "tSalesforceOutput": {
      "category": "api",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "SalesforceSink",
      "spark": "simple-salesforce library",
      "notes": "Salesforce sink"
    },
    "tSalesforceConnection": {
      "category": "api",
      "data_factory": "LinkedService_Salesforce",
      "spark": "simple-salesforce connection",
      "notes": "Salesforce connection"
    },
    "tSalesforceBulkExec": {
      "category": "api",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "simple-salesforce bulk API",
      "notes": "Salesforce bulk operations"
    },
    "tSAPInput": {
      "category": "api",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "SapTableSource",
      "spark": "pyrfc / SAP HANA JDBC",
      "notes": "SAP source"
    },
    "tSAPOutput": {
      "category": "api",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "SapSink",
      "spark": "pyrfc / SAP HANA JDBC",
      "notes": "SAP sink"
    },
    "tSAPConnection": {
      "category": "api",
      "data_factory": "LinkedService_SAP",
      "spark": "pyrfc connection config",
      "notes": "SAP connection"
    },
    "tSAPBWInput": {
      "category": "api",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "SapBwSource",
      "spark": "pyrfc BW connector",
      "notes": "SAP BW source"
    },
    "tLDAPInput": {
      "category": "api",
      "data_factory": "NotSupported",
      "spark": "python-ldap library",
      "notes": "LDAP read"
    },
    "tLDAPOutput": {
      "category": "api",
      "data_factory": "NotSupported",
      "spark": "python-ldap library",
      "notes": "LDAP write"
    },
    "tGraphQLInput": {
      "category": "api",
      "data_factory": "WebActivity_POST",
      "spark": "requests with GraphQL query",
      "notes": "GraphQL API"
    },
    "tODataInput": {
      "category": "api",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "ODataSource",
      "spark": "requests + OData URL",
      "notes": "OData source"
    },
    "tServiceNowInput": {
      "category": "api",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "ServiceNowSource",
      "spark": "requests + ServiceNow API",
      "notes": "ServiceNow source"
    },

    "_comment_transforms": "=== TRANSFORMATION COMPONENTS ===",
    "tMap": {
      "category": "transform",
      "data_factory": "MappingDataFlow_DerivedColumn_Join",
      "spark": "df.withColumn() / df.join()",
      "notes": "Complex — single tMap may map to multiple Spark operations"
    },
    "tFilter": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Filter",
      "spark": "df.filter() / df.where()",
      "notes": "Direct mapping"
    },
    "tFilterRow": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Filter",
      "spark": "df.filter() / df.where()",
      "notes": "Direct mapping"
    },
    "tAggregate": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Aggregate",
      "spark": "df.groupBy().agg()",
      "notes": "Direct mapping"
    },
    "tAggregateRow": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Aggregate",
      "spark": "df.groupBy().agg()",
      "notes": "Direct mapping"
    },
    "tSort": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Sort",
      "spark": "df.orderBy()",
      "notes": "Direct mapping"
    },
    "tSortRow": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Sort",
      "spark": "df.orderBy()",
      "notes": "Direct mapping"
    },
    "tUniqRow": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Aggregate",
      "spark": "df.dropDuplicates()",
      "notes": "Deduplicate rows"
    },
    "tJoin": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Join",
      "spark": "df1.join(df2, ...)",
      "notes": "Direct mapping"
    },
    "tUnite": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Union",
      "spark": "df1.unionByName(df2)",
      "notes": "Direct mapping"
    },
    "tSplitRow": {
      "category": "transform",
      "data_factory": "MappingDataFlow_ConditionalSplit",
      "spark": "Multiple df.filter() calls",
      "notes": "One filter per output branch"
    },
    "tXMLMap": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Flatten_DerivedColumn",
      "spark": "spark.read.format('xml') + transforms",
      "notes": "XML mapping"
    },
    "tDenormalize": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Pivot",
      "spark": "df.groupBy().pivot()",
      "notes": "Pivot/denormalize rows to columns"
    },
    "tNormalize": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Unpivot",
      "spark": "df.select(explode())",
      "notes": "Unpivot columns to rows"
    },
    "tReplace": {
      "category": "transform",
      "data_factory": "MappingDataFlow_DerivedColumn",
      "spark": "df.withColumn(regexp_replace())",
      "notes": "String replacement"
    },
    "tConvertType": {
      "category": "transform",
      "data_factory": "MappingDataFlow_DerivedColumn",
      "spark": "df.withColumn(col.cast())",
      "notes": "Type conversion"
    },
    "tReplicate": {
      "category": "transform",
      "data_factory": "MultipleSinks",
      "spark": "Write df to multiple targets",
      "notes": "Duplicate flow to multiple outputs"
    },
    "tSampleRow": {
      "category": "transform",
      "data_factory": "TopN_Percentage",
      "spark": "df.sample(fraction) / df.limit(N)",
      "notes": "Sample rows"
    },
    "tHashRow": {
      "category": "transform",
      "data_factory": "MappingDataFlow_DerivedColumn",
      "spark": "F.md5(F.concat_ws()) / F.sha2()",
      "notes": "Hash row for deduplication/comparison"
    },
    "tGenKey": {
      "category": "transform",
      "data_factory": "MappingDataFlow_SurrogateKey",
      "spark": "F.monotonically_increasing_id() / F.row_number()",
      "notes": "Surrogate key generation"
    },
    "tRowGenerator": {
      "category": "transform",
      "data_factory": "NotApplicable",
      "spark": "spark.range() / spark.createDataFrame()",
      "notes": "Test data generation"
    },
    "tFixedFlowInput": {
      "category": "transform",
      "data_factory": "NotApplicable",
      "spark": "spark.createDataFrame([...])",
      "notes": "Fixed test data"
    },
    "tExtractXMLField": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Flatten",
      "spark": "F.from_xml() / xpath extraction",
      "notes": "Extract XML fields from column"
    },
    "tExtractJSONField": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Parse",
      "spark": "F.from_json() / F.get_json_object()",
      "notes": "Extract JSON fields from column"
    },
    "tExtractRegexFields": {
      "category": "transform",
      "data_factory": "MappingDataFlow_DerivedColumn",
      "spark": "F.regexp_extract()",
      "notes": "Extract fields using regex"
    },
    "tExtractDelimitedFields": {
      "category": "transform",
      "data_factory": "MappingDataFlow_DerivedColumn",
      "spark": "F.split() + getItem()",
      "notes": "Extract delimited fields from single column"
    },
    "tFuzzyMatch": {
      "category": "transform",
      "data_factory": "NotSupported",
      "spark": "fuzzywuzzy / recordlinkage / Spark ML",
      "notes": "Fuzzy matching — requires custom logic"
    },
    "tRecordMatching": {
      "category": "transform",
      "data_factory": "NotSupported",
      "spark": "recordlinkage library",
      "notes": "Record matching — requires custom logic"
    },
    "tMatchGroup": {
      "category": "transform",
      "data_factory": "NotSupported",
      "spark": "Custom grouping logic",
      "notes": "Match + group duplicates"
    },
    "tSchemaComplianceCheck": {
      "category": "transform",
      "data_factory": "DataQualityRules",
      "spark": "Custom validation with df.filter()",
      "notes": "Schema validation"
    },
    "tFlowMeter": {
      "category": "transform",
      "data_factory": "PipelineMetrics",
      "spark": "df.count() + logging",
      "notes": "Row count / throughput metrics"
    },
    "tWindowInput": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Window",
      "spark": "Window.partitionBy().orderBy()",
      "notes": "Window functions"
    },
    "tCacheIn": {
      "category": "transform",
      "data_factory": "NotApplicable",
      "spark": "df.cache()",
      "notes": "Cache dataset for reuse"
    },
    "tCacheOut": {
      "category": "transform",
      "data_factory": "NotApplicable",
      "spark": "df.unpersist()",
      "notes": "Release cached dataset"
    },
    "tBufferInput": {
      "category": "transform",
      "data_factory": "NotApplicable",
      "spark": "df.cache() / df.persist()",
      "notes": "Buffer rows in memory"
    },
    "tBufferOutput": {
      "category": "transform",
      "data_factory": "NotApplicable",
      "spark": "df.unpersist()",
      "notes": "Flush buffer"
    },
    "tDataMasking": {
      "category": "transform",
      "data_factory": "NotSupported",
      "spark": "F.sha2() / F.regexp_replace()",
      "notes": "Data masking / anonymization"
    },
    "tFillEmptyField": {
      "category": "transform",
      "data_factory": "MappingDataFlow_DerivedColumn",
      "spark": "F.coalesce() / df.na.fill()",
      "notes": "Replace null/empty values"
    },
    "tFilterColumns": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Select",
      "spark": "df.select() / df.drop()",
      "notes": "Select/drop columns"
    },
    "tRenameColumns": {
      "category": "transform",
      "data_factory": "MappingDataFlow_DerivedColumn",
      "spark": "df.withColumnRenamed()",
      "notes": "Rename columns"
    },

    "_comment_elt": "=== ELT / PUSHDOWN COMPONENTS ===",
    "tELTInput": {
      "category": "elt",
      "data_factory": "CopyActivity",
      "spark": "spark.sql('SELECT ...')",
      "notes": "ELT input — pushdown to database"
    },
    "tELTOutput": {
      "category": "elt",
      "data_factory": "CopyActivity",
      "spark": "spark.sql('INSERT INTO ...')",
      "notes": "ELT output — pushdown to database"
    },
    "tELTMap": {
      "category": "elt",
      "data_factory": "MappingDataFlow",
      "spark": "spark.sql() with JOINs/transforms",
      "notes": "ELT mapping — pushdown transforms"
    },
    "tELTAggregate": {
      "category": "elt",
      "data_factory": "MappingDataFlow_Aggregate",
      "spark": "spark.sql() with GROUP BY",
      "notes": "ELT aggregation"
    },
    "tELTFilter": {
      "category": "elt",
      "data_factory": "MappingDataFlow_Filter",
      "spark": "spark.sql() with WHERE",
      "notes": "ELT filter"
    },
    "tELTSort": {
      "category": "elt",
      "data_factory": "MappingDataFlow_Sort",
      "spark": "spark.sql() with ORDER BY",
      "notes": "ELT sort"
    },
    "tELTUnite": {
      "category": "elt",
      "data_factory": "MappingDataFlow_Union",
      "spark": "spark.sql() with UNION ALL",
      "notes": "ELT union"
    },
    "tELTJoin": {
      "category": "elt",
      "data_factory": "MappingDataFlow_Join",
      "spark": "spark.sql() with JOIN",
      "notes": "ELT join"
    },
    "tELTDistinct": {
      "category": "elt",
      "data_factory": "MappingDataFlow_Aggregate",
      "spark": "spark.sql() with DISTINCT",
      "notes": "ELT distinct"
    },
    "tELTCreateTable": {
      "category": "elt",
      "data_factory": "ScriptActivity",
      "spark": "spark.sql('CREATE TABLE ...')",
      "notes": "ELT DDL"
    },

    "_comment_flow_control": "=== FLOW CONTROL COMPONENTS ===",
    "tRunJob": {
      "category": "flow_control",
      "data_factory": "ExecutePipeline",
      "spark": "mssparkutils.notebook.run()",
      "notes": "Child job invocation"
    },
    "tPreJob": {
      "category": "flow_control",
      "data_factory": "Pipeline_OnStart",
      "spark": "Code at beginning of notebook",
      "notes": "Pre-processing logic"
    },
    "tPostJob": {
      "category": "flow_control",
      "data_factory": "Pipeline_OnCompletion",
      "spark": "try/finally block",
      "notes": "Post-processing logic"
    },
    "tWarn": {
      "category": "flow_control",
      "data_factory": "SetVariable_WebActivity",
      "spark": "logging.warning()",
      "notes": "Warning/alert"
    },
    "tDie": {
      "category": "flow_control",
      "data_factory": "FailActivity",
      "spark": "raise Exception()",
      "notes": "Abort execution"
    },
    "tFlowToIterate": {
      "category": "flow_control",
      "data_factory": "ForEachActivity",
      "spark": "for item in df.collect():",
      "notes": "Convert flow to iteration"
    },
    "tLoop": {
      "category": "flow_control",
      "data_factory": "UntilActivity_ForEachActivity",
      "spark": "while/for loop",
      "notes": "Looping"
    },
    "tForEach": {
      "category": "flow_control",
      "data_factory": "ForEachActivity",
      "spark": "for loop",
      "notes": "ForEach iteration"
    },
    "tParallelize": {
      "category": "flow_control",
      "data_factory": "ForEachActivity_Concurrent",
      "spark": "Spark parallelism (native)",
      "notes": "Parallel execution"
    },
    "tContextLoad": {
      "category": "flow_control",
      "data_factory": "PipelineParameters",
      "spark": "Notebook parameters / config file",
      "notes": "Context variable loading"
    },
    "tIfRow": {
      "category": "flow_control",
      "data_factory": "IfConditionActivity",
      "spark": "if statement",
      "notes": "Conditional branching"
    },
    "tTimeout": {
      "category": "flow_control",
      "data_factory": "PipelineTimeout",
      "spark": "signal.alarm() / thread timeout",
      "notes": "Timeout control"
    },
    "tFlowControl": {
      "category": "flow_control",
      "data_factory": "ActivityDependencies",
      "spark": "Sequential code execution",
      "notes": "Flow control / sequencing"
    },
    "tIterateToFlow": {
      "category": "flow_control",
      "data_factory": "Variable_ForEach_Array",
      "spark": "List comprehension → DataFrame",
      "notes": "Convert iteration to flow"
    },

    "_comment_error_handling": "=== ERROR HANDLING COMPONENTS ===",
    "tCatch": {
      "category": "error_handling",
      "data_factory": "Pipeline_OnFailure",
      "spark": "try/except block",
      "notes": "Catch exceptions"
    },
    "tCatchError": {
      "category": "error_handling",
      "data_factory": "Pipeline_OnFailure",
      "spark": "try/except block",
      "notes": "Catch and handle errors"
    },
    "tLogCatcher": {
      "category": "error_handling",
      "data_factory": "Pipeline_LogAnalytics",
      "spark": "logging module + appenders",
      "notes": "Centralized log collection"
    },
    "tStatCatcher": {
      "category": "error_handling",
      "data_factory": "Pipeline_RunStatistics",
      "spark": "Custom metrics with df.count() + logging",
      "notes": "Execution statistics collection"
    },
    "tAssert": {
      "category": "error_handling",
      "data_factory": "FailActivity_OnCondition",
      "spark": "assert statement / raise on condition",
      "notes": "Assertion check"
    },
    "tAssertCatcher": {
      "category": "error_handling",
      "data_factory": "Pipeline_OnFailure",
      "spark": "try/except for AssertionError",
      "notes": "Catch assertion failures"
    },
    "tConnectionReset": {
      "category": "error_handling",
      "data_factory": "AutomaticRetryPolicy",
      "spark": "JDBC reconnection logic",
      "notes": "Reset DB connection on error"
    },

    "_comment_file_utilities": "=== FILE & DIRECTORY UTILITY COMPONENTS ===",
    "tFileExist": {
      "category": "file_utility",
      "data_factory": "GetMetadataActivity_Exists",
      "spark": "mssparkutils.fs.exists()",
      "notes": "Check file existence"
    },
    "tFileDelete": {
      "category": "file_utility",
      "data_factory": "DeleteActivity",
      "spark": "mssparkutils.fs.rm()",
      "notes": "Delete file"
    },
    "tFileCopy": {
      "category": "file_utility",
      "data_factory": "CopyActivity",
      "spark": "mssparkutils.fs.cp()",
      "notes": "Copy file"
    },
    "tFileMove": {
      "category": "file_utility",
      "data_factory": "CopyActivity_PlusDelete",
      "spark": "mssparkutils.fs.mv()",
      "notes": "Move file (copy + delete)"
    },
    "tFileRename": {
      "category": "file_utility",
      "data_factory": "CopyActivity_PlusDelete",
      "spark": "mssparkutils.fs.mv()",
      "notes": "Rename file"
    },
    "tFileList": {
      "category": "file_utility",
      "data_factory": "GetMetadataActivity_ChildItems",
      "spark": "mssparkutils.fs.ls()",
      "notes": "List files in directory"
    },
    "tFileProperties": {
      "category": "file_utility",
      "data_factory": "GetMetadataActivity",
      "spark": "mssparkutils.fs.ls()",
      "notes": "Get file properties (size, modified)"
    },
    "tFileCompare": {
      "category": "file_utility",
      "data_factory": "NotSupported",
      "spark": "Hash comparison in Python",
      "notes": "Compare two files"
    },
    "tFileArchive": {
      "category": "file_utility",
      "data_factory": "NotSupported",
      "spark": "zipfile / tarfile Python modules",
      "notes": "Archive files (zip/tar)"
    },
    "tFileUnarchive": {
      "category": "file_utility",
      "data_factory": "NotSupported",
      "spark": "zipfile / tarfile Python modules",
      "notes": "Extract archive"
    },
    "tFileTouch": {
      "category": "file_utility",
      "data_factory": "ScriptActivity",
      "spark": "mssparkutils.fs.put() with empty content",
      "notes": "Create empty file"
    },
    "tDirectoryCreate": {
      "category": "file_utility",
      "data_factory": "ScriptActivity",
      "spark": "mssparkutils.fs.mkdirs()",
      "notes": "Create directory"
    },
    "tDirectoryList": {
      "category": "file_utility",
      "data_factory": "GetMetadataActivity",
      "spark": "mssparkutils.fs.ls()",
      "notes": "List directory contents"
    },
    "tDirectoryDelete": {
      "category": "file_utility",
      "data_factory": "DeleteActivity",
      "spark": "mssparkutils.fs.rm(recursive=True)",
      "notes": "Delete directory"
    },

    "_comment_ftp_sftp": "=== FTP / SFTP COMPONENTS ===",
    "tFTPGet": {
      "category": "file_transfer",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "FtpSource",
      "spark": "ftplib Python library",
      "notes": "Download from FTP"
    },
    "tFTPPut": {
      "category": "file_transfer",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "FtpSink",
      "spark": "ftplib Python library",
      "notes": "Upload to FTP"
    },
    "tFTPConnection": {
      "category": "file_transfer",
      "data_factory": "LinkedService_FTP",
      "spark": "ftplib connection",
      "notes": "FTP connection"
    },
    "tFTPDelete": {
      "category": "file_transfer",
      "data_factory": "ScriptActivity",
      "spark": "ftplib delete",
      "notes": "Delete file on FTP"
    },
    "tFTPList": {
      "category": "file_transfer",
      "data_factory": "GetMetadataActivity",
      "spark": "ftplib.nlst()",
      "notes": "List FTP directory"
    },
    "tSFTPGet": {
      "category": "file_transfer",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "SftpSource",
      "spark": "paramiko SFTPClient",
      "notes": "Download from SFTP"
    },
    "tSFTPPut": {
      "category": "file_transfer",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "SftpSink",
      "spark": "paramiko SFTPClient.put()",
      "notes": "Upload to SFTP"
    },
    "tSFTPConnection": {
      "category": "file_transfer",
      "data_factory": "LinkedService_SFTP",
      "spark": "paramiko SSH connection",
      "notes": "SFTP connection"
    },
    "tSFTPDelete": {
      "category": "file_transfer",
      "data_factory": "ScriptActivity",
      "spark": "paramiko SFTPClient.remove()",
      "notes": "Delete file on SFTP"
    },
    "tSFTPRename": {
      "category": "file_transfer",
      "data_factory": "ScriptActivity",
      "spark": "paramiko SFTPClient.rename()",
      "notes": "Rename file on SFTP"
    },
    "tSFTPList": {
      "category": "file_transfer",
      "data_factory": "GetMetadataActivity",
      "spark": "paramiko SFTPClient.listdir()",
      "notes": "List SFTP directory"
    },

    "_comment_utilities": "=== GENERAL UTILITY COMPONENTS ===",
    "tLogRow": {
      "category": "utility",
      "data_factory": "Pipeline_monitoring",
      "spark": "display() / print()",
      "notes": "Debugging/logging"
    },
    "tSendMail": {
      "category": "utility",
      "data_factory": "WebActivity_LogicApp",
      "spark": "smtplib or Logic App webhook",
      "notes": "Email notification"
    },
    "tSystem": {
      "category": "utility",
      "data_factory": "ScriptActivity",
      "spark": "subprocess.run()",
      "notes": "Execute system command"
    },
    "tSleep": {
      "category": "utility",
      "data_factory": "WaitActivity",
      "spark": "time.sleep()",
      "notes": "Delay execution"
    },
    "tSetGlobalVar": {
      "category": "utility",
      "data_factory": "SetVariableActivity",
      "spark": "Python variable",
      "notes": "Set global variable"
    },
    "tContextDump": {
      "category": "utility",
      "data_factory": "PipelineParameterLogging",
      "spark": "print(locals())",
      "notes": "Dump context variables"
    },
    "tSSH": {
      "category": "utility",
      "data_factory": "NotSupported",
      "spark": "paramiko SSH client",
      "notes": "SSH command execution"
    },
    "tSSHRemote": {
      "category": "utility",
      "data_factory": "NotSupported",
      "spark": "paramiko SSH client",
      "notes": "Remote SSH execution"
    },
    "tLibraryLoad": {
      "category": "utility",
      "data_factory": "NotApplicable",
      "spark": "%pip install / sc.addPyFile()",
      "notes": "Load external library"
    },
    "tTimestamp": {
      "category": "utility",
      "data_factory": "PipelineVariable_Timestamp",
      "spark": "datetime.utcnow()",
      "notes": "Get current timestamp"
    },

    "_comment_custom_code": "=== CUSTOM CODE COMPONENTS ===",
    "tJava": {
      "category": "custom_code",
      "data_factory": "NotSupported_UseSparkInstead",
      "spark": "Python code block",
      "notes": "Custom Java — manually translate to Python"
    },
    "tJavaRow": {
      "category": "custom_code",
      "data_factory": "NotSupported_UseSparkInstead",
      "spark": "df.withColumn() with UDF",
      "notes": "Row-level Java — translate to Spark UDF"
    },
    "tJavaFlex": {
      "category": "custom_code",
      "data_factory": "NotSupported_UseSparkInstead",
      "spark": "Custom PySpark with init/main/end blocks",
      "notes": "Flexible Java — translate to Python pattern"
    },
    "tGroovy": {
      "category": "custom_code",
      "data_factory": "ScriptActivity",
      "spark": "Python code block",
      "notes": "Groovy script — translate to Python"
    },
    "tPythonRow": {
      "category": "custom_code",
      "data_factory": "NotApplicable",
      "spark": "Direct PySpark / UDF",
      "notes": "Already Python — direct migration"
    },
    "tJavaScript": {
      "category": "custom_code",
      "data_factory": "NotSupported",
      "spark": "Python equivalent",
      "notes": "JavaScript — translate to Python"
    },
    "tSetKeystore": {
      "category": "custom_code",
      "data_factory": "KeyVaultReference",
      "spark": "mssparkutils.credentials.getSecret()",
      "notes": "Keystore / key vault"
    },

    "_comment_hash_components": "=== HASH / IN-MEMORY LOOKUP COMPONENTS ===",
    "tHashInput": {
      "category": "transform",
      "data_factory": "NotApplicable",
      "spark": "broadcast(df) / df.cache()",
      "notes": "Read from in-memory hash table — replace with broadcast join or cached DataFrame"
    },
    "tHashOutput": {
      "category": "transform",
      "data_factory": "NotApplicable",
      "spark": "df.cache() / df.persist(StorageLevel.MEMORY_ONLY)",
      "notes": "Write to in-memory hash table — replace with cached/broadcast DataFrame"
    },

    "_comment_cdc": "=== CHANGE DATA CAPTURE (CDC) COMPONENTS ===",
    "tOracleCDC": {
      "category": "cdc",
      "data_factory": "CopyActivity_Incremental",
      "spark": "spark.read.jdbc() with watermark filter",
      "notes": "Oracle CDC via LogMiner/XStream — use watermark or Fabric Mirroring"
    },
    "tMysqlCDC": {
      "category": "cdc",
      "data_factory": "CopyActivity_Incremental",
      "spark": "spark.read.jdbc() with watermark filter",
      "notes": "MySQL CDC via binlog — use watermark or Debezium + Event Hub"
    },
    "tMSSqlCDC": {
      "category": "cdc",
      "data_factory": "CopyActivity_Incremental",
      "spark": "spark.read.jdbc() with watermark filter",
      "notes": "SQL Server CDC — use Fabric Mirroring or watermark pattern"
    },
    "tPostgresqlCDC": {
      "category": "cdc",
      "data_factory": "CopyActivity_Incremental",
      "spark": "spark.read.jdbc() with watermark filter",
      "notes": "PostgreSQL CDC via logical replication — use Fabric Mirroring or Debezium"
    },
    "tDBCDC": {
      "category": "cdc",
      "data_factory": "CopyActivity_Incremental",
      "spark": "spark.read.jdbc() with watermark filter",
      "notes": "Generic CDC — use incremental load with high watermark column"
    },

    "_comment_db_specific_ops": "=== DATABASE-SPECIFIC OPERATION COMPONENTS ===",
    "tOracleRow": {
      "category": "db_operation",
      "data_factory": "ScriptActivity",
      "spark": "spark.sql('DML statement')",
      "notes": "Execute arbitrary SQL on Oracle"
    },
    "tPostgresqlRow": {
      "category": "db_operation",
      "data_factory": "ScriptActivity",
      "spark": "spark.sql('DML statement')",
      "notes": "Execute arbitrary SQL on PostgreSQL"
    },
    "tMSSqlRow": {
      "category": "db_operation",
      "data_factory": "ScriptActivity",
      "spark": "spark.sql('DML statement')",
      "notes": "Execute arbitrary SQL on SQL Server"
    },
    "tMysqlRow": {
      "category": "db_operation",
      "data_factory": "ScriptActivity",
      "spark": "spark.sql('DML statement')",
      "notes": "Execute arbitrary SQL on MySQL"
    },
    "tDB2Row": {
      "category": "db_operation",
      "data_factory": "ScriptActivity",
      "spark": "spark.sql('DML statement')",
      "notes": "Execute arbitrary SQL on DB2"
    },
    "tOracleSP": {
      "category": "db_operation",
      "data_factory": "StoredProcedureActivity",
      "spark": "spark.sql('CALL ...')",
      "notes": "Oracle stored procedure"
    },
    "tMSSqlSP": {
      "category": "db_operation",
      "data_factory": "StoredProcedureActivity",
      "spark": "spark.sql('EXEC ...')",
      "notes": "SQL Server stored procedure"
    },
    "tMysqlSP": {
      "category": "db_operation",
      "data_factory": "StoredProcedureActivity",
      "spark": "spark.sql('CALL ...')",
      "notes": "MySQL stored procedure"
    },
    "tPostgresqlSP": {
      "category": "db_operation",
      "data_factory": "StoredProcedureActivity",
      "spark": "spark.sql('CALL ...')",
      "notes": "PostgreSQL stored procedure / function"
    },
    "tMysqlBulkExec": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "df.write.jdbc(batchsize=N)",
      "notes": "MySQL LOAD DATA INFILE — bulk insert"
    },
    "tMSSqlBulkExec": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "df.write.jdbc(batchsize=N)",
      "notes": "SQL Server BCP / BULK INSERT"
    },
    "tPostgresqlBulkExec": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "df.write.jdbc(batchsize=N)",
      "notes": "PostgreSQL COPY — bulk insert"
    },
    "tTeradataBulkExec": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "df.write.jdbc(batchsize=N)",
      "notes": "Teradata FastLoad/MultiLoad — bulk insert"
    },
    "tSnowflakeBulkExec": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "df.write.format('snowflake')",
      "notes": "Snowflake COPY INTO — bulk insert"
    },
    "tOracleOutputBulk": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "df.write.jdbc(batchsize=N)",
      "notes": "Oracle SQL*Loader style bulk output"
    },
    "tPostgresqlOutputBulk": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "df.write.jdbc(batchsize=N)",
      "notes": "PostgreSQL COPY style bulk output"
    },
    "tMSSqlOutputBulk": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "df.write.jdbc(batchsize=N)",
      "notes": "SQL Server BCP style bulk output"
    },
    "tOracleInputBulk": {
      "category": "db_operation",
      "data_factory": "CopyActivity_BulkMode",
      "spark": "spark.read.jdbc(fetchsize=N)",
      "notes": "Oracle bulk input with external tables"
    },
    "tMSSqlCommit": {
      "category": "db_operation",
      "data_factory": "AutoCommit",
      "spark": "Explicit commit / auto in Spark",
      "notes": "SQL Server transaction commit"
    },
    "tMysqlCommit": {
      "category": "db_operation",
      "data_factory": "AutoCommit",
      "spark": "Explicit commit / auto in Spark",
      "notes": "MySQL transaction commit"
    },
    "tPostgresqlCommit": {
      "category": "db_operation",
      "data_factory": "AutoCommit",
      "spark": "Explicit commit / auto in Spark",
      "notes": "PostgreSQL transaction commit"
    },
    "tMSSqlClose": {
      "category": "db_operation",
      "data_factory": "Automatic",
      "spark": "Connection auto-close",
      "notes": "SQL Server close connection"
    },
    "tMysqlClose": {
      "category": "db_operation",
      "data_factory": "Automatic",
      "spark": "Connection auto-close",
      "notes": "MySQL close connection"
    },
    "tPostgresqlClose": {
      "category": "db_operation",
      "data_factory": "Automatic",
      "spark": "Connection auto-close",
      "notes": "PostgreSQL close connection"
    },
    "tDB2Connection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Db2",
      "spark": "JDBC connection properties",
      "notes": "DB2 connection definition"
    },
    "tTeradataConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Teradata",
      "spark": "JDBC connection properties",
      "notes": "Teradata connection definition"
    },
    "tSnowflakeConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Snowflake",
      "spark": "Snowflake connector config",
      "notes": "Snowflake connection definition"
    },
    "tSybaseConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Odbc",
      "spark": "JDBC connection properties",
      "notes": "Sybase / SAP ASE connection definition"
    },
    "tRedshiftConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_AmazonRedshift",
      "spark": "JDBC connection properties",
      "notes": "Amazon Redshift connection definition"
    },
    "tHiveConnection": {
      "category": "db_connection",
      "data_factory": "NotApplicable_NativeToSpark",
      "spark": "spark.sql() — native",
      "notes": "Hive connection — native in Spark/Lakehouse"
    },
    "tSQLiteConnection": {
      "category": "db_connection",
      "data_factory": "NotSupported",
      "spark": "JDBC connection properties",
      "notes": "SQLite connection"
    },
    "tInformixConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Informix",
      "spark": "JDBC connection properties",
      "notes": "Informix connection definition"
    },

    "_comment_extra_databases": "=== ADDITIONAL DATABASE COMPONENTS ===",
    "tVerticaInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "VerticaSource",
      "spark": "spark.read.jdbc()",
      "notes": "Vertica source"
    },
    "tVerticaOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "VerticaSink",
      "spark": "df.write.jdbc()",
      "notes": "Vertica sink"
    },
    "tVerticaConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Vertica",
      "spark": "JDBC connection properties",
      "notes": "Vertica connection"
    },
    "tNetezzaInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "NetezzaSource",
      "spark": "spark.read.jdbc()",
      "notes": "Netezza source"
    },
    "tNetezzaOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "NetezzaSink",
      "spark": "df.write.jdbc()",
      "notes": "Netezza sink"
    },
    "tNetezzaConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Netezza",
      "spark": "JDBC connection properties",
      "notes": "Netezza connection"
    },
    "tGreenplumInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "GreenplumSource",
      "spark": "spark.read.jdbc()",
      "notes": "Greenplum source"
    },
    "tGreenplumOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "GreenplumSink",
      "spark": "df.write.jdbc()",
      "notes": "Greenplum sink"
    },
    "tGreenplumConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Greenplum",
      "spark": "JDBC connection properties",
      "notes": "Greenplum connection"
    },
    "tAS400Input": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "Db2Source",
      "spark": "spark.read.jdbc()",
      "notes": "AS/400 (DB2 for i) source"
    },
    "tAS400Output": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "Db2Sink",
      "spark": "df.write.jdbc()",
      "notes": "AS/400 (DB2 for i) sink"
    },
    "tAS400Connection": {
      "category": "db_connection",
      "data_factory": "LinkedService_Db2",
      "spark": "JDBC connection properties (jt400 driver)",
      "notes": "AS/400 connection"
    },
    "tSAPHanaInput": {
      "category": "db_input",
      "data_factory": "CopyActivity",
      "data_factory_source_type": "SapHanaSource",
      "spark": "spark.read.jdbc()",
      "notes": "SAP HANA source"
    },
    "tSAPHanaOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "SapHanaSink",
      "spark": "df.write.jdbc()",
      "notes": "SAP HANA sink"
    },
    "tSAPHanaConnection": {
      "category": "db_connection",
      "data_factory": "LinkedService_SapHana",
      "spark": "JDBC connection properties (ngdbc driver)",
      "notes": "SAP HANA connection"
    },
    "tSybaseOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "OdbcSink",
      "spark": "df.write.jdbc()",
      "notes": "Sybase / SAP ASE sink"
    },
    "tInformixOutput": {
      "category": "db_output",
      "data_factory": "CopyActivity",
      "data_factory_sink_type": "InformixSink",
      "spark": "df.write.jdbc()",
      "notes": "Informix sink"
    },
    "tAccessInput": {
      "category": "db_input",
      "data_factory": "NotSupported",
      "spark": "spark.read.jdbc() (UCanAccess driver)",
      "notes": "MS Access source — convert to CSV/Parquet first"
    },
    "tAccessOutput": {
      "category": "db_output",
      "data_factory": "NotSupported",
      "spark": "df.write.jdbc() (UCanAccess driver)",
      "notes": "MS Access sink — convert to CSV/Parquet first"
    },
    "tSQLiteOutput": {
      "category": "db_output",
      "data_factory": "NotSupported",
      "spark": "df.write.jdbc()",
      "notes": "SQLite sink — use JDBC driver"
    },

    "_comment_bigdata": "=== BIG DATA / HADOOP COMPONENTS ===",
    "tHiveCreateTable": {
      "category": "db_operation",
      "data_factory": "ScriptActivity_DDL",
      "spark": "spark.sql('CREATE TABLE ...')",
      "notes": "Create Hive/Lakehouse table"
    },
    "tHiveLoad": {
      "category": "db_operation",
      "data_factory": "CopyActivity",
      "spark": "spark.sql('INSERT INTO ... SELECT ...')",
      "notes": "Load data into Hive/Lakehouse table"
    },
    "tHiveRow": {
      "category": "db_operation",
      "data_factory": "ScriptActivity",
      "spark": "spark.sql('DML statement')",
      "notes": "Execute HiveQL statement"
    },
    "tSqoopImport": {
      "category": "bigdata",
      "data_factory": "CopyActivity",
      "spark": "spark.read.jdbc() — Sqoop is deprecated, use Spark JDBC",
      "notes": "Sqoop import — replaced by Spark JDBC direct read"
    },
    "tSqoopExport": {
      "category": "bigdata",
      "data_factory": "CopyActivity",
      "spark": "df.write.jdbc() — Sqoop is deprecated, use Spark JDBC",
      "notes": "Sqoop export — replaced by Spark JDBC direct write"
    },
    "tSparkConfiguration": {
      "category": "bigdata",
      "data_factory": "NotApplicable",
      "spark": "SparkSession.builder.config(key, value)",
      "notes": "Spark configuration — set via Fabric Spark environment"
    },
    "tImpalaInput": {
      "category": "db_input",
      "data_factory": "NotSupported",
      "spark": "spark.read.jdbc() (Impala JDBC driver)",
      "notes": "Impala source"
    },
    "tImpalaOutput": {
      "category": "db_output",
      "data_factory": "NotSupported",
      "spark": "df.write.jdbc() (Impala JDBC driver)",
      "notes": "Impala sink"
    },
    "tImpalaConnection": {
      "category": "db_connection",
      "data_factory": "NotSupported",
      "spark": "JDBC connection properties (Impala driver)",
      "notes": "Impala connection"
    },
    "tPigLoad": {
      "category": "bigdata",
      "data_factory": "NotApplicable",
      "spark": "Rewrite as spark.read — Pig is deprecated",
      "notes": "Pig load — rewrite as PySpark"
    },
    "tPigMap": {
      "category": "bigdata",
      "data_factory": "NotApplicable",
      "spark": "Rewrite as PySpark transformations",
      "notes": "Pig mapping — rewrite as PySpark"
    },
    "tPigStore": {
      "category": "bigdata",
      "data_factory": "NotApplicable",
      "spark": "Rewrite as df.write — Pig is deprecated",
      "notes": "Pig store — rewrite as PySpark"
    },
    "tMapReduceInput": {
      "category": "bigdata",
      "data_factory": "NotApplicable",
      "spark": "Rewrite as spark.read — MapReduce is deprecated",
      "notes": "MapReduce input — rewrite as PySpark"
    },
    "tMapReduceOutput": {
      "category": "bigdata",
      "data_factory": "NotApplicable",
      "spark": "Rewrite as df.write — MapReduce is deprecated",
      "notes": "MapReduce output — rewrite as PySpark"
    },
    "tSparkInput": {
      "category": "bigdata",
      "data_factory": "NotApplicable",
      "spark": "spark.read — native",
      "notes": "Spark input — already native"
    },
    "tSparkOutput": {
      "category": "bigdata",
      "data_factory": "NotApplicable",
      "spark": "df.write — native",
      "notes": "Spark output — already native"
    },

    "_comment_xml_advanced": "=== ADVANCED XML / XSLT COMPONENTS ===",
    "tXSLT": {
      "category": "transform",
      "data_factory": "NotSupported",
      "spark": "lxml.etree.XSLT() in Python",
      "notes": "XSLT transformation — use Python lxml for stylesheet transforms"
    },
    "tAdvancedFileOutputXML": {
      "category": "file_output",
      "data_factory": "DataFlow_XmlSink",
      "spark": "lxml.etree.Element() — build XML tree in Python",
      "notes": "Advanced hierarchical XML output with nested elements"
    },
    "tWebServiceOutput": {
      "category": "api",
      "data_factory": "WebActivity_POST",
      "spark": "Flask / FastAPI endpoint",
      "notes": "Web service response — use notebook as REST endpoint or Logic App"
    },

    "_comment_mq": "=== IBM MQ SERIES COMPONENTS ===",
    "tMQSeriesInput": {
      "category": "messaging",
      "data_factory": "NotSupported",
      "spark": "pymqi Python library",
      "notes": "IBM MQ consumer"
    },
    "tMQSeriesOutput": {
      "category": "messaging",
      "data_factory": "NotSupported",
      "spark": "pymqi Python library",
      "notes": "IBM MQ producer"
    },
    "tMQSeriesConnection": {
      "category": "messaging",
      "data_factory": "NotSupported",
      "spark": "pymqi connection manager",
      "notes": "IBM MQ connection"
    },

    "_comment_mdm_dq": "=== MDM & DATA QUALITY COMPONENTS ===",
    "tMDMInput": {
      "category": "api",
      "data_factory": "WebActivity_REST",
      "spark": "requests + Fabric API",
      "notes": "Talend MDM input — use REST API or Fabric data pipeline"
    },
    "tMDMOutput": {
      "category": "api",
      "data_factory": "WebActivity_REST",
      "spark": "requests + Fabric API",
      "notes": "Talend MDM output"
    },
    "tPatternCheck": {
      "category": "transform",
      "data_factory": "MappingDataFlow_DerivedColumn",
      "spark": "F.rlike() / F.regexp_extract()",
      "notes": "Regex pattern validation"
    },
    "tValueCount": {
      "category": "transform",
      "data_factory": "MappingDataFlow_Aggregate",
      "spark": "df.groupBy().count()",
      "notes": "Value distribution count"
    },
    "tSurvivorshipRules": {
      "category": "transform",
      "data_factory": "NotSupported",
      "spark": "Custom PySpark survivorship logic",
      "notes": "Record survivorship — pick best record from duplicates"
    },
    "tStandardize": {
      "category": "transform",
      "data_factory": "MappingDataFlow_DerivedColumn",
      "spark": "F.when() / lookup tables / UDF",
      "notes": "Data standardization / cleansing"
    },
    "tMatchPairing": {
      "category": "transform",
      "data_factory": "NotSupported",
      "spark": "recordlinkage library",
      "notes": "Record matching / pairing"
    }
  }
}
